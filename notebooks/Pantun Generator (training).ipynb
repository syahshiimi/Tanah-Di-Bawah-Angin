{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "precise-replica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440, 2)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"/Users/syahrulanuar/miniforge3/envs/ganimg/puisi-pantun-generator/data/pantun.csv\")\n",
    "print(data.shape)\n",
    "data.head()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cahya/gpt2-small-indonesian-522M\")\n",
    "  \n",
    "model = AutoModelWithLMHead.from_pretrained(\"cahya/gpt2-small-indonesian-522M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prospective-prototype",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train:  418\n",
      "Len test:  22\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data.teks.values,\n",
    "                               test_size=0.05,\n",
    "                               random_state=31) \n",
    "\n",
    "print('Len train: ', len(train))\n",
    "print('Len test: ', len(test))\n",
    "\n",
    "with open('train.txt', 'w+') as f:\n",
    "    for text in train:\n",
    "        f.write('<BOS> ' + repr(text)[1:-1] + ' <EOS>\\n')\n",
    "        \n",
    "with open('test.txt', 'w+') as f:\n",
    "    for text in test:\n",
    "        f.write('<BOS> ' + repr(text)[1:-1] + ' <EOS>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stretch-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 23:49:12 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "02/10/2021 23:49:12 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../gpt2-pantun, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_23-49-12_Syahruls-MacBook-Air.local, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../gpt2-pantun, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=0)\n",
      "/Users/syahrulanuar/miniforge3/envs/ganimg/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:966: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "/Users/syahrulanuar/miniforge3/envs/ganimg/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/Users/syahrulanuar/miniforge3/envs/ganimg/lib/python3.8/site-packages/transformers/trainer.py:702: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 43/106 [03:36<06:34,  6.26s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!python3 ../puisi-pantun-generator/src/run_language_modeling.py \\\n",
    "--output_dir='../gpt2-pantun' \\\n",
    "--model_type=gpt2 \\\n",
    "--model_name_or_path='cahya/gpt2-small-indonesian-522M' \\\n",
    "--do_train \\\n",
    "--train_data_file='train.txt' \\\n",
    "--do_eval \\\n",
    "--eval_data_file='test.txt' \\\n",
    "--per_device_train_batch_size=8 \\\n",
    "--per_device_eval_batch_size=8 \\\n",
    "--line_by_line \\\n",
    "--learning_rate 5e-5 \\\n",
    "--num_train_epochs=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-arrangement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-treasury",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
